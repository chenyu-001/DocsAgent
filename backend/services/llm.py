"""LLM client utilities."""
from openai import OpenAI
from api.config import settings
from loguru import logger


class LLMClient:
    """Wrapper around an OpenAI-compatible chat completion API."""

    def __init__(self):
        if not settings.LLM_API_KEY:
            raise ValueError("LLM_API_KEY is not configured")

        self.client = OpenAI(
            api_key=settings.LLM_API_KEY,
            base_url=settings.LLM_API_BASE,
        )

    def generate_answer(self, question: str, context: str) -> str:
        """Generate an answer using provided context snippets."""
        # Estimate question complexity to adjust answer length
        is_complex = len(question) > 30 or 'å¦‚ä½•' in question or 'æ­¥éª¤' in question or 'è¯¦ç»†' in question or 'æµç¨‹' in question
        max_length = "500-800" if is_complex else "200-400"
        max_tokens = 1200 if is_complex else 600

        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ DocsAgent ä¼ä¸šçŸ¥è¯†åº“çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºŽæä¾›ç²¾å‡†ã€ç»“æž„åŒ–çš„ç­”æ¡ˆã€‚\n\n"
                    "**æ ¸å¿ƒåŽŸåˆ™ï¼š**\n"
                    "1. ç›´æŽ¥å›žç­”ï¼Œä¸è¦å•°å—¦ - ç”¨æˆ·æ—¶é—´å®è´µ\n"
                    "2. çªå‡ºé‡ç‚¹ï¼Œä¸è¦å¹³é“º - å…ˆè¯´æœ€é‡è¦çš„\n"
                    "3. ç»“æž„æ¸…æ™°ï¼Œæ˜“äºŽæ‰«è¯» - ä½¿ç”¨æ ‡é¢˜å’Œåˆ—è¡¨\n"
                    "4. å¼•ç”¨æ¥æºï¼Œå¯è¿½æº¯ - å¿…é¡»æ ‡æ³¨æ–‡æ¡£ç¼–å·\n\n"
                    "**ç¦æ­¢çš„è¡Œä¸ºï¼š**\n"
                    "âŒ ä¸è¦å†™é•¿ç¯‡å¤§è®ºï¼Œä¸è¦å•°å—¦é‡å¤\n"
                    "âŒ ä¸è¦å¹³é“ºæ‰€æœ‰ä¿¡æ¯ï¼Œè¦æç‚¼æ ¸å¿ƒ\n"
                    "âŒ ä¸è¦ç¼–é€ å†…å®¹ï¼ŒåªåŸºäºŽæ–‡æ¡£å›žç­”\n"
                    "âŒ ä¸è¦å¿½ç•¥æ¥æºå¼•ç”¨"
                ),
            },
            {
                "role": "user",
                "content": (
                    f"**é—®é¢˜ï¼š** {question}\n\n"
                    f"**æ–‡æ¡£ç‰‡æ®µï¼š**\n{context}\n\n"
                    "---\n\n"
                    f"**è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼å›žç­”ï¼ˆæ€»é•¿åº¦ {max_length} å­—ï¼‰ï¼š**\n\n"
                    "## ðŸŽ¯ æ ¸å¿ƒç­”æ¡ˆ\n"
                    "[ä¸€å¥è¯ç›´æŽ¥å›žç­”é—®é¢˜ï¼Œ30-80å­—]\n\n"
                    "## ðŸ“‹ å…³é”®è¦ç‚¹\n"
                    "- **è¦ç‚¹1**ï¼š[ç®€æ´æè¿°] `[æ–‡æ¡£1]`\n"
                    "- **è¦ç‚¹2**ï¼š[ç®€æ´æè¿°] `[æ–‡æ¡£2]`\n"
                    "- **è¦ç‚¹3**ï¼š[ç®€æ´æè¿°] `[æ–‡æ¡£3]`\n\n"
                    "## ðŸ’¡ è¡¥å……è¯´æ˜Žï¼ˆå¯é€‰ï¼‰\n"
                    "[å¦‚æœ‰å¿…è¦ï¼Œè¡¥å……é‡è¦ç»†èŠ‚]\n\n"
                    "---\n\n"
                    "**æ ¼å¼è¦æ±‚ï¼š**\n"
                    "1. å¿…é¡»åŒ…å«\"æ ¸å¿ƒç­”æ¡ˆ\"å’Œ\"å…³é”®è¦ç‚¹\"ä¸¤ä¸ªéƒ¨åˆ†\n"
                    "2. å¼•ç”¨æ ¼å¼ï¼š`[æ–‡æ¡£1]` `[æ–‡æ¡£2]`ï¼ˆæ•°å­—å¯¹åº”æ–‡æ¡£ç‰‡æ®µç¼–å·ï¼‰\n"
                    "3. æ ¹æ®é—®é¢˜å¤æ‚åº¦è°ƒæ•´é•¿åº¦ï¼š\n"
                    "   - ç®€å•é—®é¢˜ï¼š200-400å­—ï¼Œ3-5ä¸ªè¦ç‚¹\n"
                    "   - å¤æ‚é—®é¢˜ï¼ˆå¦‚ä½•ã€æ­¥éª¤ã€è¯¦ç»†ï¼‰ï¼š500-800å­—ï¼Œ5-8ä¸ªè¦ç‚¹\n"
                    "4. å¦‚æžœæ–‡æ¡£ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œç›´æŽ¥è¯´\"æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\"\n"
                    "5. ä½¿ç”¨åŠ ç²— (**) çªå‡ºå…³é”®è¯"
                ),
            },
        ]

        response = self.client.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=messages,
            temperature=0.3,  # Lower temperature for more focused answers
            max_tokens=max_tokens,  # Adaptive based on question complexity
            timeout=settings.LLM_TIMEOUT,
        )

        answer = response.choices[0].message.content or ""
        return answer.strip()

    def generate_summary(self, text: str, filename: str) -> str:
        """
        Generate structured summary for a document

        Args:
            text: Document full text (will be truncated if too long)
            filename: Document filename for context

        Returns:
            Structured summary in markdown format
        """
        # Truncate text if too long (keep first 8000 chars for summary)
        max_chars = 8000
        if len(text) > max_chars:
            text = text[:max_chars] + "\n\n[æ–‡æ¡£å†…å®¹è¿‡é•¿ï¼Œä»…åŸºäºŽå‰ 8000 å­—ç”Ÿæˆæ‘˜è¦]"

        messages = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ DocsAgent æ–‡æ¡£æ‘˜è¦ä¸“å®¶ï¼Œæ“…é•¿æç‚¼æ–‡æ¡£æ ¸å¿ƒä¿¡æ¯ã€‚\n\n"
                    "**ä½ çš„ä»»åŠ¡ï¼š**\n"
                    "ä¸ºæ–‡æ¡£ç”Ÿæˆç»“æž„åŒ–æ‘˜è¦ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£æ–‡æ¡£å†…å®¹å’Œä»·å€¼ã€‚\n\n"
                    "**æ ¸å¿ƒåŽŸåˆ™ï¼š**\n"
                    "1. ç²¾ç‚¼å‡†ç¡® - æ¯ä¸ªå­—éƒ½æœ‰ä»·å€¼\n"
                    "2. çªå‡ºé‡ç‚¹ - æ ¸å¿ƒå†…å®¹ä¼˜å…ˆ\n"
                    "3. ç»“æž„æ¸…æ™° - ä¾¿äºŽå¿«é€Ÿæ‰«è¯»\n"
                    "4. å®žç”¨å¯¼å‘ - å¸®åŠ©ç”¨æˆ·åˆ¤æ–­æ–‡æ¡£ä»·å€¼"
                ),
            },
            {
                "role": "user",
                "content": (
                    f"**æ–‡æ¡£åç§°ï¼š** {filename}\n\n"
                    f"**æ–‡æ¡£å†…å®¹ï¼š**\n{text}\n\n"
                    "---\n\n"
                    "**è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼ç”Ÿæˆæ‘˜è¦ï¼ˆæ€»é•¿åº¦ 150-200 å­—ï¼‰ï¼š**\n\n"
                    "**ðŸ“„ æ–‡æ¡£ä¸»é¢˜**\n"
                    "[ä¸€å¥è¯æ¦‚æ‹¬æ–‡æ¡£æ ¸å¿ƒå†…å®¹ï¼Œ20-30å­—]\n\n"
                    "**ðŸŽ¯ æ ¸å¿ƒè¦ç‚¹**\n"
                    "- è¦ç‚¹ 1ï¼ˆ15-20å­—ï¼‰\n"
                    "- è¦ç‚¹ 2ï¼ˆ15-20å­—ï¼‰\n"
                    "- è¦ç‚¹ 3ï¼ˆ15-20å­—ï¼‰\n\n"
                    "**ðŸ’¼ é€‚ç”¨åœºæ™¯**\n"
                    "- åœºæ™¯ 1ï¼ˆ15-20å­—ï¼‰\n"
                    "- åœºæ™¯ 2ï¼ˆ15-20å­—ï¼‰\n\n"
                    "---\n\n"
                    "**æ ¼å¼è¦æ±‚ï¼š**\n"
                    "1. æ€»å­—æ•° 150-200 å­—ï¼ˆä¸å« emoji å’Œæ ‡é¢˜ï¼‰\n"
                    "2. æ ¸å¿ƒè¦ç‚¹å¿…é¡»æç‚¼æœ€é‡è¦çš„ 3 æ¡\n"
                    "3. é€‚ç”¨åœºæ™¯å¿…é¡»å†™æ˜Žæ–‡æ¡£çš„ä½¿ç”¨ä»·å€¼\n"
                    "4. æ¯æ¡éƒ½è¦ç®€æ´ï¼Œä¸è¶…è¿‡ 20 å­—\n"
                    "5. ç¦æ­¢å‡ºçŽ°\"æœ¬æ–‡æ¡£\"ã€\"è¯¥æ–‡æ¡£\"ç­‰å†—ä½™è¡¨è¿°\n"
                    "6. ç¦æ­¢ç¼–é€ æ–‡æ¡£ä¸­æ²¡æœ‰çš„å†…å®¹"
                ),
            },
        ]

        response = self.client.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=messages,
            temperature=0.3,
            max_tokens=500,
            timeout=settings.LLM_TIMEOUT,
        )

        summary = response.choices[0].message.content or ""
        return summary.strip()


_llm_client: LLMClient | None = None


def get_llm_client() -> LLMClient:
    """Get or create a singleton LLM client instance."""
    global _llm_client
    if _llm_client is None:
        try:
            _llm_client = LLMClient()
        except Exception as exc:  # noqa: BLE001
            logger.error(f"Failed to initialize LLM client: {exc}")
            raise
    return _llm_client
